{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| ID|    name|age|numFriends|\n",
      "+---+--------+---+----------+\n",
      "|  0|    Will| 33|       385|\n",
      "|  1|Jean-Luc| 26|         2|\n",
      "|  2|    Hugh| 55|       221|\n",
      "|  3|  Deanna| 40|       465|\n",
      "|  4|   Quark| 68|        21|\n",
      "+---+--------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]),name=str(fields[1]), \\\n",
    "        age=int(fields[2]),numFriends=int(fields[3]))\n",
    "    \n",
    "lines = spark.sparkContext.textFile(\"../resources/sources/fakefriends.csv\")\n",
    "people = lines.map(mapper)\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "teenagers = spark.sql('SELECT * FROM people WHERE age >= 13 and age <= 19')\n",
    "teenagers.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by age\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31|    8|\n",
      "| 65|    5|\n",
      "| 53|    7|\n",
      "| 34|    6|\n",
      "| 28|   10|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 44|   12|\n",
      "| 22|    7|\n",
      "| 47|    9|\n",
      "| 52|   11|\n",
      "| 40|   17|\n",
      "| 20|    5|\n",
      "| 57|   12|\n",
      "| 54|   13|\n",
      "| 48|   10|\n",
      "| 19|   11|\n",
      "| 64|   12|\n",
      "| 41|    9|\n",
      "| 43|    7|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "    \n",
    "people = spark.read.option('header','true').option('inferSchema','true').csv(\"../resources/sources/fakefriends.csv\")\n",
    "\n",
    "print('Here is our inferred schema:')\n",
    "people.printSchema()\n",
    "\n",
    "print('Lets display the name column:')\n",
    "people.select('name').show()\n",
    "\n",
    "print('Filter out anyone over 21:')\n",
    "people.filter(people.age < 21).show()\n",
    "\n",
    "print('Group by age')\n",
    "people.groupBy('age').count().show()\n",
    "\n",
    "print('Make everyone 10 years older:')\n",
    "people.select(people.name,people.age + 10).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|avg_friends|\n",
      "+---+-----------+\n",
      "| 18|     343.38|\n",
      "| 19|     213.27|\n",
      "| 20|      165.0|\n",
      "| 21|     350.88|\n",
      "| 22|     206.43|\n",
      "| 23|      246.3|\n",
      "| 24|      233.8|\n",
      "| 25|     197.45|\n",
      "| 26|     242.06|\n",
      "| 27|     228.13|\n",
      "| 28|      209.1|\n",
      "| 29|     215.92|\n",
      "| 30|     235.82|\n",
      "| 31|     267.25|\n",
      "| 32|     207.91|\n",
      "| 33|     325.33|\n",
      "| 34|      245.5|\n",
      "| 35|     211.63|\n",
      "| 36|      246.6|\n",
      "| 37|     249.33|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import try_avg,round\n",
    "\n",
    "spark = SparkSession.builder.appName('numberOfFriendsByAge').getOrCreate()\n",
    "    \n",
    "people = spark.read.option('header','true').option('inferSchema','true').csv(\"../resources/sources/fakefriends.csv\")\n",
    "\n",
    "people.createOrReplaceTempView('people')\n",
    "\n",
    "filteredPeople = spark.sql('select age,number_of_friends from people')\n",
    "\n",
    "finalDataframe = filteredPeople.groupBy('age').agg(round(try_avg('number_of_friends'),2).alias('avg_friends')).sort('age').show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+\n",
      "|  stationID|min(temperature)|temperature|\n",
      "+-----------+----------------+-----------+\n",
      "|ITE00100554|          -148.0|      -14.8|\n",
      "|EZE00100082|          -135.0|      -13.5|\n",
      "+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun \n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName('minimumTemperatures').getOrCreate()\n",
    "\n",
    "schema = StructType([\\\n",
    "    StructField('stationID',StringType(),True), \\\n",
    "    StructField('date',IntegerType(),True), \\\n",
    "    StructField('measureType',StringType(),True), \\\n",
    "    StructField('temperature',FloatType(),True)])\n",
    "\n",
    "df = spark.read.schema(schema).csv('../resources/sources/1800.csv')\n",
    "\n",
    "minTemps = df.filter(df.measureType == 'TMIN')\n",
    "\n",
    "stationTemps = minTemps.select('stationID','temperature')\n",
    "\n",
    "minTempsByStation = stationTemps.groupby('stationID').min('temperature')\n",
    "\n",
    "minTempsByStationF = minTempsByStation.withColumn('temperature',fun.round(fun.col('min(temperature)') * 0.1, 2))\n",
    "\n",
    "minTempsByStationF.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|userID|totalAmount|\n",
      "+------+-----------+\n",
      "|    68|    6375.45|\n",
      "|    73|     6206.2|\n",
      "|    39|    6193.11|\n",
      "|    54|    6065.39|\n",
      "|    71|    5995.66|\n",
      "|     2|    5994.59|\n",
      "|    97|    5977.19|\n",
      "|    46|    5963.11|\n",
      "|    42|    5696.84|\n",
      "|    59|    5642.89|\n",
      "|    41|    5637.62|\n",
      "|     0|    5524.95|\n",
      "|     8|    5517.24|\n",
      "|    85|    5503.43|\n",
      "|    61|    5497.48|\n",
      "|    32|    5496.05|\n",
      "|    58|    5437.73|\n",
      "|    63|    5415.15|\n",
      "|    15|    5413.51|\n",
      "|     6|    5397.88|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName('aggregateByCustomer').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userID\", IntegerType(), True),\n",
    "    StructField(\"itemID\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = (spark.read\n",
    "           .option(\"header\", \"false\")\n",
    "           .schema(schema)\n",
    "           .csv(\"../resources/sources/customer-orders.csv\")\n",
    "     )\n",
    "\n",
    "transformedDf = (\n",
    "    df.groupBy(\"userID\")\n",
    "      .agg(fun.round(fun.sum(\"amount\"), 2).alias(\"totalAmount\"))\n",
    "      .sort('totalAmount',ascending=False)\n",
    ")\n",
    "\n",
    "transformedDf.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m fun \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, IntegerType, DoubleType\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# <-- Add this so Spark uses all local cores\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpopularMovieDf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#df = (spark.read\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#           .text(\"../resources/sources/u.data\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#df.show()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")   # <-- Add this so Spark uses all local cores\n",
    "         .appName(\"popularMovieDf\")\n",
    "         .getOrCreate())\n",
    "\n",
    "#df = (spark.read\n",
    "#           .text(\"../resources/sources/u.data\")\n",
    "#     )\n",
    "#\n",
    "#df.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
