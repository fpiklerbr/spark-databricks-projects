{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| ID|    name|age|numFriends|\n",
      "+---+--------+---+----------+\n",
      "|  0|    Will| 33|       385|\n",
      "|  1|Jean-Luc| 26|         2|\n",
      "|  2|    Hugh| 55|       221|\n",
      "|  3|  Deanna| 40|       465|\n",
      "|  4|   Quark| 68|        21|\n",
      "+---+--------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]),name=str(fields[1]), \\\n",
    "        age=int(fields[2]),numFriends=int(fields[3]))\n",
    "    \n",
    "lines = spark.sparkContext.textFile(\"../resources/sources/fakefriends.csv\")\n",
    "people = lines.map(mapper)\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "teenagers = spark.sql('SELECT * FROM people WHERE age >= 13 and age <= 19')\n",
    "teenagers.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by age\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31|    8|\n",
      "| 65|    5|\n",
      "| 53|    7|\n",
      "| 34|    6|\n",
      "| 28|   10|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 44|   12|\n",
      "| 22|    7|\n",
      "| 47|    9|\n",
      "| 52|   11|\n",
      "| 40|   17|\n",
      "| 20|    5|\n",
      "| 57|   12|\n",
      "| 54|   13|\n",
      "| 48|   10|\n",
      "| 19|   11|\n",
      "| 64|   12|\n",
      "| 41|    9|\n",
      "| 43|    7|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "    \n",
    "people = spark.read.option('header','true').option('inferSchema','true').csv(\"../resources/sources/fakefriends.csv\")\n",
    "\n",
    "print('Here is our inferred schema:')\n",
    "people.printSchema()\n",
    "\n",
    "print('Lets display the name column:')\n",
    "people.select('name').show()\n",
    "\n",
    "print('Filter out anyone over 21:')\n",
    "people.filter(people.age < 21).show()\n",
    "\n",
    "print('Group by age')\n",
    "people.groupBy('age').count().show()\n",
    "\n",
    "print('Make everyone 10 years older:')\n",
    "people.select(people.name,people.age + 10).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|avg_friends|\n",
      "+---+-----------+\n",
      "| 18|     343.38|\n",
      "| 19|     213.27|\n",
      "| 20|      165.0|\n",
      "| 21|     350.88|\n",
      "| 22|     206.43|\n",
      "| 23|      246.3|\n",
      "| 24|      233.8|\n",
      "| 25|     197.45|\n",
      "| 26|     242.06|\n",
      "| 27|     228.13|\n",
      "| 28|      209.1|\n",
      "| 29|     215.92|\n",
      "| 30|     235.82|\n",
      "| 31|     267.25|\n",
      "| 32|     207.91|\n",
      "| 33|     325.33|\n",
      "| 34|      245.5|\n",
      "| 35|     211.63|\n",
      "| 36|      246.6|\n",
      "| 37|     249.33|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import try_avg,round\n",
    "\n",
    "spark = SparkSession.builder.appName('numberOfFriendsByAge').getOrCreate()\n",
    "    \n",
    "people = spark.read.option('header','true').option('inferSchema','true').csv(\"../resources/sources/fakefriends.csv\")\n",
    "\n",
    "people.createOrReplaceTempView('people')\n",
    "\n",
    "filteredPeople = spark.sql('select age,number_of_friends from people')\n",
    "\n",
    "finalDataframe = filteredPeople.groupBy('age').agg(round(try_avg('number_of_friends'),2).alias('avg_friends')).sort('age').show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+\n",
      "|  stationID|min(temperature)|temperature|\n",
      "+-----------+----------------+-----------+\n",
      "|ITE00100554|          -148.0|      -14.8|\n",
      "|EZE00100082|          -135.0|      -13.5|\n",
      "+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun \n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName('minimumTemperatures').getOrCreate()\n",
    "\n",
    "schema = StructType([\\\n",
    "    StructField('stationID',StringType(),True), \\\n",
    "    StructField('date',IntegerType(),True), \\\n",
    "    StructField('measureType',StringType(),True), \\\n",
    "    StructField('temperature',FloatType(),True)])\n",
    "\n",
    "df = spark.read.schema(schema).csv('../resources/sources/1800.csv')\n",
    "\n",
    "minTemps = df.filter(df.measureType == 'TMIN')\n",
    "\n",
    "stationTemps = minTemps.select('stationID','temperature')\n",
    "\n",
    "minTempsByStation = stationTemps.groupby('stationID').min('temperature')\n",
    "\n",
    "minTempsByStationF = minTempsByStation.withColumn('temperature',fun.round(fun.col('min(temperature)') * 0.1, 2))\n",
    "\n",
    "minTempsByStationF.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|userID|totalAmount|\n",
      "+------+-----------+\n",
      "|    68|    6375.45|\n",
      "|    73|     6206.2|\n",
      "|    39|    6193.11|\n",
      "|    54|    6065.39|\n",
      "|    71|    5995.66|\n",
      "|     2|    5994.59|\n",
      "|    97|    5977.19|\n",
      "|    46|    5963.11|\n",
      "|    42|    5696.84|\n",
      "|    59|    5642.89|\n",
      "|    41|    5637.62|\n",
      "|     0|    5524.95|\n",
      "|     8|    5517.24|\n",
      "|    85|    5503.43|\n",
      "|    61|    5497.48|\n",
      "|    32|    5496.05|\n",
      "|    58|    5437.73|\n",
      "|    63|    5415.15|\n",
      "|    15|    5413.51|\n",
      "|     6|    5397.88|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName('aggregateByCustomer').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userID\", IntegerType(), True),\n",
    "    StructField(\"itemID\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = (spark.read\n",
    "           .option(\"header\", \"false\")\n",
    "           .schema(schema)\n",
    "           .csv(\"../resources/sources/customer-orders.csv\")\n",
    "     )\n",
    "\n",
    "transformedDf = (\n",
    "    df.groupBy(\"userID\")\n",
    "      .agg(fun.round(fun.sum(\"amount\"), 2).alias(\"totalAmount\"))\n",
    "      .sort('totalAmount',ascending=False)\n",
    ")\n",
    "\n",
    "transformedDf.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+\n",
      "|movieID|numRatings|       movieName|\n",
      "+-------+----------+----------------+\n",
      "|     50|       583|Star Wars (1977)|\n",
      "+-------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"popularMovieDf\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .text(\"../resources/ml-100k/u.data\")\n",
    ")\n",
    "\n",
    "dfMovieNames = (\n",
    "    spark.read\n",
    "    .text(\"../resources/ml-100k/u.item\")\n",
    ")\n",
    "\n",
    "df_formatted = df.select(\n",
    "    fun.split(df.value, \"\\t\").getItem(0).cast(StringType()).alias(\"userID\"),\n",
    "    fun.split(df.value, \"\\t\").getItem(1).cast(StringType()).alias(\"movieID\"),\n",
    "    fun.split(df.value, \"\\t\").getItem(2).cast(IntegerType()).alias(\"rating\"),\n",
    "    fun.split(df.value, \"\\t\").getItem(3).cast(IntegerType()).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "dfMovieNames_formatted = dfMovieNames.select(\n",
    "    fun.split(dfMovieNames.value, \"\\\\|\").getItem(0).cast(StringType()).alias(\"movieID\"),\n",
    "    fun.split(dfMovieNames.value, \"\\\\|\").getItem(1).cast(StringType()).alias(\"movieName\")\n",
    ")\n",
    "\n",
    "df_ratings_count = df_formatted.groupBy(\"movieID\").count().withColumnRenamed(\"count\", \"numRatings\")\n",
    "max_ratings = df_ratings_count.agg(fun.max(\"numRatings\").alias(\"max_numRatings\")).collect()[0][\"max_numRatings\"]\n",
    "df_most_rated = df_ratings_count.filter(fun.col(\"numRatings\") == max_ratings)\n",
    "joinedDf = df_most_rated.join(dfMovieNames_formatted, on=\"movieID\", how=\"left\")\n",
    "\n",
    "joinedDf.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|  id|count|            heroName|\n",
      "+----+-----+--------------------+\n",
      "| 859| 1937|     CAPTAIN AMERICA|\n",
      "|5306| 1745|SPIDER-MAN/PETER PAR|\n",
      "|2664| 1532|IRON MAN/TONY STARK |\n",
      "|5716| 1429|THING/BENJAMIN J. GR|\n",
      "|6306| 1397|    WOLVERINE/LOGAN |\n",
      "|3805| 1389|MR. FANTASTIC/REED R|\n",
      "|2557| 1374|HUMAN TORCH/JOHNNY S|\n",
      "|4898| 1348|SCARLET WITCH/WANDA |\n",
      "|5736| 1292|THOR/DR. DONALD BLAK|\n",
      "| 403| 1283|BEAST/HENRY &HANK& P|\n",
      "|6066| 1266|             VISION |\n",
      "|2650| 1247|INVISIBLE WOMAN/SUE |\n",
      "|2399| 1179|                HAWK|\n",
      "|1289| 1107|CYCLOPS/SCOTT SUMMER|\n",
      "|5467| 1098|STORM/ORORO MUNROE S|\n",
      "| 133| 1097|ANGEL/WARREN KENNETH|\n",
      "|6148| 1096|WASP/JANET VAN DYNE |\n",
      "| 154| 1095|ANT-MAN/DR. HENRY J.|\n",
      "|5046| 1083|SHE-HULK/JENNIFER WA|\n",
      "|1602| 1082|DR. STRANGE/STEPHEN |\n",
      "+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"popularSuperhero\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .text(\"../resources/marvel-dataset/Marvel_Graph.txt\")\n",
    ")\n",
    "\n",
    "dfNames = (\n",
    "    spark.read\n",
    "    .text(\"../resources/marvel-dataset/Marvel_Names.txt\")\n",
    ")\n",
    "\n",
    "df_splitted = df.select(fun.split(fun.col(\"value\"), \" \").alias(\"split_values\"))\n",
    "df_exploded = df_splitted.select(fun.explode(fun.col(\"split_values\")).alias(\"id\"))\n",
    "df_id_count = df_exploded.groupBy(\"id\").count().orderBy(fun.desc(\"count\"))\n",
    "df_id_count = df_id_count.filter(fun.col('id') != '')\n",
    "\n",
    "dfNamesFormatted = (\n",
    "    dfNames\n",
    "    .select(\n",
    "        fun.split(fun.col(\"value\"), \" \", 2).getItem(0).alias(\"id\"),\n",
    "        fun.split(fun.col(\"value\"), \" \", 2).getItem(1).alias(\"hero_name_raw\")\n",
    "    )\n",
    "    # Remove surrounding quotes from the hero name\n",
    "    .withColumn(\"heroName\",\n",
    "                fun.regexp_replace(\"hero_name_raw\", '\"', \"\"))  \n",
    "    .drop(\"hero_name_raw\")\n",
    ")\n",
    "\n",
    "finalDf = df_id_count.join(dfNamesFormatted,how='left',on='id').sort('count',ascending=False)\n",
    "\n",
    "finalDf.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 19:47:12 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar movies for Titanic (1997)\n",
      "Shawshank Redemption, The (1994)\tscore: 0.9774914218747776\tstrength: 98\n",
      "Braveheart (1995)\tscore: 0.9773906047511945\tstrength: 114\n",
      "In the Line of Fire (1993)\tscore: 0.9750576027310787\tstrength: 70\n",
      "Sling Blade (1996)\tscore: 0.974991844395603\tstrength: 54\n",
      "Edge, The (1997)\tscore: 0.9739215608821148\tstrength: 67\n",
      "Die Hard (1988)\tscore: 0.9738113931276092\tstrength: 87\n",
      "Primal Fear (1996)\tscore: 0.9736181709820277\tstrength: 87\n",
      "Good Will Hunting (1997)\tscore: 0.9715894131404956\tstrength: 160\n",
      "Fugitive, The (1993)\tscore: 0.9714155141969335\tstrength: 119\n",
      "Glory (1989)\tscore: 0.9713991588484581\tstrength: 71\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import sys\n",
    "\n",
    "def computeCosineSimilarity(spark, data):\n",
    "    # Compute xx, xy and yy columns\n",
    "    pairScores = data \\\n",
    "      .withColumn(\"xx\", func.col(\"rating1\") * func.col(\"rating1\")) \\\n",
    "      .withColumn(\"yy\", func.col(\"rating2\") * func.col(\"rating2\")) \\\n",
    "      .withColumn(\"xy\", func.col(\"rating1\") * func.col(\"rating2\")) \n",
    "\n",
    "    # Compute numerator, denominator and numPairs columns\n",
    "    calculateSimilarity = pairScores \\\n",
    "      .groupBy(\"movie1\", \"movie2\") \\\n",
    "      .agg( \\\n",
    "        func.sum(func.col(\"xy\")).alias(\"numerator\"), \\\n",
    "        (func.sqrt(func.sum(func.col(\"xx\"))) * func.sqrt(func.sum(func.col(\"yy\")))).alias(\"denominator\"), \\\n",
    "        func.count(func.col(\"xy\")).alias(\"numPairs\")\n",
    "      )\n",
    "\n",
    "    # Calculate score and select only needed columns (movie1, movie2, score, numPairs)\n",
    "    result = calculateSimilarity \\\n",
    "      .withColumn(\"score\", \\\n",
    "        func.when(func.col(\"denominator\") != 0, func.col(\"numerator\") / func.col(\"denominator\")) \\\n",
    "          .otherwise(0) \\\n",
    "      ).select(\"movie1\", \"movie2\", \"score\", \"numPairs\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Get movie name by given movie id \n",
    "def getMovieName(movieNames, movieId):\n",
    "    result = movieNames.filter(func.col(\"movieID\") == movieId) \\\n",
    "        .select(\"movieTitle\").collect()[0]\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieSimilarities\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "movieNamesSchema = StructType([ \\\n",
    "                               StructField(\"movieID\", IntegerType(), True), \\\n",
    "                               StructField(\"movieTitle\", StringType(), True) \\\n",
    "                               ])\n",
    "    \n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "    \n",
    "    \n",
    "# Create a broadcast dataset of movieID and movieTitle.\n",
    "# Apply ISO-885901 charset\n",
    "movieNames = spark.read \\\n",
    "      .option(\"sep\", \"|\") \\\n",
    "      .option(\"charset\", \"ISO-8859-1\") \\\n",
    "      .schema(movieNamesSchema) \\\n",
    "      .csv(\"../resources/ml-100k/u.item\")\n",
    "\n",
    "# Load up movie data as dataset\n",
    "movies = spark.read \\\n",
    "      .option(\"sep\", \"\\t\") \\\n",
    "      .schema(moviesSchema) \\\n",
    "      .csv(\"../resources/ml-100k/u.data\")\n",
    "\n",
    "\n",
    "ratings = movies.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "# Emit every movie rated together by the same user.\n",
    "# Self-join to find every combination.\n",
    "# Select movie pairs and rating pairs\n",
    "moviePairs = ratings.alias(\"ratings1\") \\\n",
    "      .join(ratings.alias(\"ratings2\"), (func.col(\"ratings1.userId\") == func.col(\"ratings2.userId\")) \\\n",
    "            & (func.col(\"ratings1.movieId\") < func.col(\"ratings2.movieId\"))) \\\n",
    "      .select(func.col(\"ratings1.movieId\").alias(\"movie1\"), \\\n",
    "        func.col(\"ratings2.movieId\").alias(\"movie2\"), \\\n",
    "        func.col(\"ratings1.rating\").alias(\"rating1\"), \\\n",
    "        func.col(\"ratings2.rating\").alias(\"rating2\"))\n",
    "\n",
    "\n",
    "moviePairSimilarities = computeCosineSimilarity(spark, moviePairs).cache()\n",
    "\n",
    "if (len(sys.argv) > 1):\n",
    "    scoreThreshold = 0.97\n",
    "    coOccurrenceThreshold = 50.0\n",
    "\n",
    "    movieID = 313\n",
    "\n",
    "    # Filter for movies with this sim that are \"good\" as defined by\n",
    "    # our quality thresholds above\n",
    "    filteredResults = moviePairSimilarities.filter( \\\n",
    "        ((func.col(\"movie1\") == movieID) | (func.col(\"movie2\") == movieID)) & \\\n",
    "          (func.col(\"score\") > scoreThreshold) & (func.col(\"numPairs\") > coOccurrenceThreshold))\n",
    "\n",
    "    # Sort by quality score.\n",
    "    results = filteredResults.sort(func.col(\"score\").desc()).take(10)\n",
    "    \n",
    "    print (\"Top 10 similar movies for \" + getMovieName(movieNames, movieID))\n",
    "    \n",
    "    for result in results:\n",
    "        # Display the similarity result that isn't the movie we're looking at\n",
    "        similarMovieID = result.movie1\n",
    "        if (similarMovieID == movieID):\n",
    "          similarMovieID = result.movie2\n",
    "        \n",
    "        print(getMovieName(movieNames, similarMovieID) + \"\\tscore: \" \\\n",
    "              + str(result.score) + \"\\tstrength: \" + str(result.numPairs))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
