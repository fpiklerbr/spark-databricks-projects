{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/14 20:11:45 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/14 20:11:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/14 20:11:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6110\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"../sources/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the file fakefriends.csv to calculate the average friends by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 325.3333333333333)\n",
      "(26, 242.05882352941177)\n",
      "(55, 295.53846153846155)\n",
      "(40, 250.8235294117647)\n",
      "(68, 269.6)\n",
      "(59, 220.0)\n",
      "(37, 249.33333333333334)\n",
      "(54, 278.0769230769231)\n",
      "(38, 193.53333333333333)\n",
      "(27, 228.125)\n",
      "(53, 222.85714285714286)\n",
      "(57, 258.8333333333333)\n",
      "(56, 306.6666666666667)\n",
      "(43, 230.57142857142858)\n",
      "(36, 246.6)\n",
      "(22, 206.42857142857142)\n",
      "(35, 211.625)\n",
      "(45, 309.53846153846155)\n",
      "(60, 202.71428571428572)\n",
      "(67, 214.625)\n",
      "(19, 213.27272727272728)\n",
      "(30, 235.8181818181818)\n",
      "(51, 302.14285714285717)\n",
      "(25, 197.45454545454547)\n",
      "(21, 350.875)\n",
      "(42, 303.5)\n",
      "(49, 184.66666666666666)\n",
      "(48, 281.4)\n",
      "(50, 254.6)\n",
      "(39, 169.28571428571428)\n",
      "(32, 207.9090909090909)\n",
      "(58, 116.54545454545455)\n",
      "(64, 281.3333333333333)\n",
      "(31, 267.25)\n",
      "(52, 340.6363636363636)\n",
      "(24, 233.8)\n",
      "(20, 165.0)\n",
      "(62, 220.76923076923077)\n",
      "(41, 268.55555555555554)\n",
      "(44, 282.1666666666667)\n",
      "(69, 235.2)\n",
      "(65, 298.2)\n",
      "(61, 256.22222222222223)\n",
      "(28, 209.1)\n",
      "(66, 276.44444444444446)\n",
      "(46, 223.69230769230768)\n",
      "(29, 215.91666666666666)\n",
      "(18, 343.375)\n",
      "(47, 233.22222222222223)\n",
      "(34, 245.5)\n",
      "(63, 384.0)\n",
      "(23, 246.3)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "if sc:\n",
    "    sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age,numFriends)\n",
    "\n",
    "lines = sc.textFile(\"../resources/sources/fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "results = averagesByAge.collect()\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes the temperature for the year 1800, from the csv file 1800.csv, and calculates the minimum temperature for the year 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "if sc:\n",
    "    sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"temperaturesAnalysis\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationId = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationId,entryType,temperature)\n",
    "\n",
    "lines = sc.textFile(\"../resources/sources/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: 'TMIN' in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0],x[2]))\n",
    "minYearTemps = stationTemps.reduceByKey(lambda x,y: min(x,y))\n",
    "\n",
    "results = minYearTemps.collect()\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script finds the number of times a specific word appears on the book inside the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import re\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the user provided the word as an argument\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python textFind.py <word>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # The word we are searching for\n",
    "    word_to_find = 'the'#sys.argv[1]\n",
    "\n",
    "\n",
    "    # Using a regex to remove all non-alphanumeric characters\n",
    "    def normalize_word(word):\n",
    "        # Remove all special characters, leaving only letters and digits\n",
    "        cleaned = re.sub(r'[^A-Za-z0-9]+', '', word)\n",
    "        return cleaned.lower()\n",
    "    \n",
    "    # Stop any existing SparkContext\n",
    "    if 'sc' in globals():\n",
    "        sc.stop()\n",
    "\n",
    "    conf = SparkConf().setMaster(\"local\").setAppName(\"textFind\")\n",
    "    sc = SparkContext(conf = conf)\n",
    "\n",
    "    lines = sc.textFile('../resources/sources/Book.txt')\n",
    "    words = lines.flatMap(lambda x: x.split())\n",
    "    matchedWords = words.filter(lambda x: normalize_word(x).lower() == word_to_find.lower())\n",
    "    results = matchedWords.count()\n",
    "    print(results)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
